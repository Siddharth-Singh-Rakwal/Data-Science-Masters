{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e915f44-5701-45ab-a82b-1f97d64f1551",
   "metadata": {},
   "source": [
    "# Assignment | 19th March 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565635f-d4c7-44ec-9adc-9ad0c3752d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6869062b-aef4-4ab4-8f48-1c4c8e28184d",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "Ans.\n",
    "\n",
    "Min-Max scaling is a data normalization technique used in data preprocessing. It scales the features of a dataset to a fixed range of values (usually between 0 and 1) based on the minimum and maximum values of the original data.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "- X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "where X is a feature value, X_min is the minimum value of that feature, and X_max is the maximum value of that feature.\n",
    "\n",
    "Min-Max scaling is useful when the range of feature values in a dataset varies widely. By scaling the values to a common range, we can ensure that each feature is equally important in the analysis and avoid issues with the features that have a larger range of values dominating the analysis.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset of house prices that ranges from $50,000 to $1,000,000. We also have a feature for the number of bedrooms, which ranges from 1 to 5. Since the range of values for these two features is different, we could apply Min-Max scaling to normalize them.\n",
    "\n",
    "To do this, we would first find the minimum and maximum values for each feature. Let's say the minimum and maximum values for the house prices are $50,000 and $1,000,000, respectively, and the minimum and maximum values for the number of bedrooms are 1 and 5, respectively.\n",
    "\n",
    "Then, we would apply the Min-Max scaling formula to each value in the dataset. For example, suppose we have a house that costs $500,000 and has 3 bedrooms. The Min-Max scaled value for the house price would be:\n",
    "\n",
    "- X_scaled = ($500,000 - $50,000) / ($1,000,000 - $50,000) = 0.4878\n",
    "\n",
    "And the Min-Max scaled value for the number of bedrooms would be:\n",
    "\n",
    "- X_scaled = (3 - 1) / (5 - 1) = 0.5\n",
    "\n",
    "By scaling the values in this way, we can compare and analyze the two features on an equal footing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdffa5aa-12ad-483a-923d-b051b0387db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a400b8f-475e-4c87-a2c8-c4863f499c1c",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "Ans.\n",
    "\n",
    "The Unit Vector technique is another data normalization technique used in feature scaling. It scales the feature values to a unit vector, meaning that the Euclidean norm of the vector is 1. This technique is also known as L2 normalization.\n",
    "\n",
    "The formula for Unit Vector scaling is:\n",
    "- X_scaled = X / ||X||\n",
    "\n",
    "where X is a feature vector and ||X|| is the Euclidean norm of that vector.\n",
    "\n",
    "Unit Vector scaling is useful when we want to preserve the direction of the feature vectors while scaling them to a common scale. It is particularly useful in machine learning algorithms that use distance measures such as K-nearest neighbors (KNN) or Support Vector Machines (SVM).\n",
    "\n",
    "Here's an example to illustrate the application of Unit Vector scaling:\n",
    "\n",
    "Suppose we have a dataset of movie ratings where each movie is rated on a scale of 1 to 5 in five different categories: action, drama, comedy, horror, and romance. We want to use these ratings to recommend similar movies to users.\n",
    "\n",
    "To use these ratings in a KNN algorithm, we need to scale the ratings to a common scale while preserving the direction of the vectors. We can do this using Unit Vector scaling.\n",
    "\n",
    "First, we would construct a feature vector for each movie that contains its ratings in each category. For example, suppose we have a movie with the following ratings:\n",
    "\n",
    "- Action: 4\n",
    "- Drama: 2\n",
    "- Comedy: 3\n",
    "- Horror: 1\n",
    "- Romance: 5\n",
    "\n",
    "We can represent this movie as a feature vector:\n",
    "\n",
    "- X = [4, 2, 3, 1, 5]\n",
    "\n",
    "Then, we would apply the Unit Vector scaling formula to this vector:\n",
    "\n",
    "- X_scaled = X / ||X|| = [0.5976, 0.2988, 0.4482, 0.1494, 0.7468]\n",
    "\n",
    "By scaling the feature vector in this way, we have preserved the direction of the vector while scaling it to a common scale. We can now use this feature vector in a KNN algorithm to recommend similar movies to users based on their ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704d400-dfb7-4031-a6bc-6b8df78486b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "511f6483-3627-47c3-ba8d-3bef4fe6927e",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "Ans. \n",
    "\n",
    "PCA (Principal Component Analysis) is a widely used technique for dimensionality reduction. It involves transforming a high-dimensional dataset into a lower-dimensional representation while retaining the most important information in the data.\n",
    "\n",
    "The PCA algorithm identifies the principal components of a dataset, which are linear combinations of the original features that capture the most variation in the data. These components are ordered by the amount of variance they explain, with the first component explaining the most variance and so on. By selecting a subset of these components, we can reduce the dimensionality of the dataset while preserving most of the variation in the data.\n",
    "\n",
    "PCA is used in dimensionality reduction because it can help to reduce the computational complexity of a model by reducing the number of features in the dataset. This can also help to avoid overfitting and improve the generalization performance of a model.\n",
    "\n",
    "Here's an example to illustrate the application of PCA in dimensionality reduction:\n",
    "\n",
    "Suppose we have a dataset of images, where each image is represented as a high-dimensional vector of pixel values. Each image has 1,000 pixels, so our dataset has 1,000 features. We want to use this dataset to train a machine learning model to classify the images into different categories.\n",
    "\n",
    "Before training the model, we can use PCA to reduce the dimensionality of the dataset. First, we would calculate the covariance matrix of the dataset, which captures the relationships between the different features. Then, we would use the eigendecomposition of the covariance matrix to identify the principal components of the dataset.\n",
    "\n",
    "Suppose we find that the first 100 principal components capture 95% of the variation in the data. We can then use these 100 components as a lower-dimensional representation of the dataset, reducing the number of features from 1,000 to 100.\n",
    "\n",
    "We can then use this reduced dataset to train a machine learning model to classify the images. By reducing the dimensionality of the dataset in this way, we have reduced the computational complexity of the model and improved its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319adc84-897c-416f-b8ce-a6995deac8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6e232c8-72c3-49f2-9bf8-f386fbb092fc",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Ans.\n",
    "\n",
    "PCA and feature extraction are closely related concepts, as PCA can be used as a feature extraction technique.\n",
    "\n",
    "Feature extraction involves transforming raw data into a set of features that are more informative and relevant for a particular task, such as classification or clustering. The goal is to reduce the dimensionality of the data and extract the most useful information from it.\n",
    "\n",
    "PCA can be used as a feature extraction technique by transforming the original features of a dataset into a new set of features that capture the most important variation in the data. These new features are linear combinations of the original features and are ordered by the amount of variance they explain. By selecting a subset of these new features, we can reduce the dimensionality of the dataset while preserving most of the important information in the data.\n",
    "\n",
    "Here's an example to illustrate the use of PCA for feature extraction:\n",
    "\n",
    "Suppose we have a dataset of handwritten digits, where each digit is represented as a 28x28 grayscale image. Each pixel in the image is a feature, so the dataset has 784 features. We want to use this dataset to train a machine learning model to classify the digits.\n",
    "\n",
    "Before training the model, we can use PCA to extract a set of features from the images. First, we would vectorize each image into a 1D array of length 784. Then, we would apply PCA to the dataset to identify the principal components of the images.\n",
    "\n",
    "Suppose we find that the first 50 principal components capture 90% of the variation in the data. We can then use these 50 components as a new set of features for the dataset. Each new feature is a linear combination of the original pixel values, and together they capture the most important variation in the images.\n",
    "\n",
    "We can then use this reduced set of features to train a machine learning model to classify the digits. By using PCA to extract a smaller set of informative features, we have reduced the dimensionality of the dataset and improved the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383fd48-5566-4bbb-9109-84d603cea52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c5f0488-d63d-4870-be17-e2c1c0f9d6b3",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "Ans.\n",
    "\n",
    "In order to use Min-Max scaling to preprocess the data for a recommendation system for a food delivery service, we would first need to identify which features to scale. In this case, the relevant features could include price, rating, and delivery time, as mentioned in the question.\n",
    "\n",
    "Once we have identified the relevant features, we can apply Min-Max scaling to normalize their values. Min-Max scaling involves scaling the values of a feature so that they fall within a specified range, typically between 0 and 1.\n",
    "\n",
    "To apply Min-Max scaling, we would first determine the minimum and maximum values of each feature in the dataset. We would then use these values to scale the values of each feature so that they fall within the range [0,1]. The formula for Min-Max scaling is:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "For example, suppose the minimum and maximum values of the price feature in our dataset are $5 and $50, respectively, and we want to scale the price values to fall within the range [0,1]. If a particular item in the dataset has a price of $15, its scaled value would be:\n",
    "\n",
    "scaled_price = ($15 - $5) / ($50 - $5) = 0.25\n",
    "\n",
    "We would apply the same formula to each item in the dataset for each feature we want to scale.\n",
    "\n",
    "After applying Min-Max scaling, the values of each feature will be normalized to fall within the range [0,1], making it easier to compare and combine them when building a recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7378a79-ae01-413e-8d00-47526ced4866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48612f22-785e-4aba-bd4a-02685dfa627f",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "Ans. \n",
    "\n",
    "PCA can be a useful technique for reducing the dimensionality of a large dataset with many features, such as the one in the stock price prediction project. Here's how we can use PCA to achieve this:\n",
    "\n",
    "1. Standardize the data: The first step is to standardize the data, meaning we need to normalize the values of each feature so that they have a mean of 0 and a variance of 1. This is important because PCA is sensitive to the scaling of the features, and standardization ensures that each feature is given equal importance.\n",
    "\n",
    "2. Calculate the covariance matrix: Next, we calculate the covariance matrix of the standardized data. The covariance matrix represents the relationships between each pair of features and is used to identify the principal components of the data.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues: We then compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions of maximum variance in the data, and the eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "4. Select the principal components: We can then select a subset of the eigenvectors with the highest eigenvalues to use as the principal components. These principal components are the new features that capture the most important variation in the data. The number of principal components we choose to keep depends on how much variance we want to preserve in the data.\n",
    "\n",
    "5. Transform the data: Finally, we transform the original data into a new dataset that consists of the selected principal components. Each observation in the new dataset is a linear combination of the original features, weighted by their corresponding coefficients in the selected principal components.\n",
    "\n",
    "By using PCA to reduce the dimensionality of the stock price prediction dataset, we can avoid the curse of dimensionality and improve the performance of our model by reducing noise and focusing on the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a110d-a0a9-4ee5-9197-3adc1d9635ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94818569-f90a-41d4-ba71-ae43046d4327",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "\n",
    "Ans.\n",
    "\n",
    "To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1, we need to follow the formula for Min-Max scaling:\n",
    "\n",
    "scaled_value = 2 * (value - min_value) / (max_value - min_value) - 1\n",
    "\n",
    "First, we determine the minimum and maximum values of the dataset, which are 1 and 20, respectively. We then apply the formula to each value in the dataset to scale the values to the desired range:\n",
    "\n",
    "- For value 1: scaled_value = 2 * (1 - 1) / (20 - 1) - 1 = -1\n",
    "- For value 5: scaled_value = 2 * (5 - 1) / (20 - 1) - 1 = -0.2\n",
    "- For value 10: scaled_value = 2 * (10 - 1) / (20 - 1) - 1 = 0.2\n",
    "- For value 15: scaled_value = 2 * (15 - 1) / (20 - 1) - 1 = 0.6\n",
    "- For value 20: scaled_value = 2 * (20 - 1) / (20 - 1) - 1 = 1\n",
    "\n",
    "Therefore, the Min-Max scaled values of the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are [-1, -0.2, 0.2, 0.6, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb819a20-7dd0-4c2f-8362-c759cbde9089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c24e041-ae63-4ce6-ab38-8aa773cba3b5",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "Ans. \n",
    "\n",
    "To perform feature extraction using PCA on the dataset containing the features [height, weight, age, gender, blood pressure], we need to follow these steps:\n",
    "\n",
    "- Standardize the data: We first need to standardize the data to ensure that each feature has a mean of 0 and a variance of 1.\n",
    "- Compute the covariance matrix: We then compute the covariance matrix of the standardized data, which represents the relationships between each pair of features.\n",
    "- Compute the eigenvectors and eigenvalues: We then compute the eigenvectors and eigenvalues of the covariance matrix, which represent the directions of maximum variance and the amount of variance explained by each eigenvector.\n",
    "\n",
    "- Select the principal components: We can then select a subset of the eigenvectors with the highest eigenvalues to use as the principal components. The number of principal components we choose to keep depends on how much variance we want to preserve in the data.\n",
    "\n",
    "To decide how many principal components to retain, we can examine the proportion of variance explained by each principal component and choose the number that captures a sufficient amount of variation in the data. One common approach is to retain enough principal components to explain at least 80% of the total variance in the data.\n",
    "\n",
    "In practice, the number of principal components to retain can also depend on the specific use case and the tradeoff between model complexity and performance.\n",
    "\n",
    "Therefore, in the case of the dataset containing the features [height, weight, age, gender, blood pressure], the number of principal components to retain would depend on the proportion of variance explained by each principal component. It's not possible to determine this without performing PCA on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573bec0-d171-4639-885f-2c37110ef713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

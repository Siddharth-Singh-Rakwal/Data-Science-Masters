{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e5f2e9-d00f-4c8a-92f0-7605145447aa",
   "metadata": {},
   "source": [
    "# Assignment | Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7503d9-fa1d-4aa9-ab15-8e457f6c0353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "affd3efa-c441-4a26-a251-a48b844d02f8",
   "metadata": {},
   "source": [
    "Objective: Assess understanding of weight initialization techniques in artificial neural networks. Evaluate\n",
    "the impact of different initialization methods on model performance. Enhance knowledge of weight\n",
    "initialization's role in improving convergence and avoiding vanishing/exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af80d5a-8db8-4f1b-b09d-a6a781e48100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6465f410-3877-4f6b-853d-8a5f6b24678f",
   "metadata": {},
   "source": [
    "## Part-1 Understanding Weight Initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed63556-1052-4b74-908f-ca73da2807f8",
   "metadata": {},
   "source": [
    "1. Explain the importance of weight initialization in artificial neural networks. WhE is it necessarE to initialize the weights carefully?\n",
    "\n",
    "2. Describe the challenges associated with improper weight initialization. How do these issues affect model training and convergence?\n",
    "\n",
    "3. Discuss the concept of variance and how it relates to weight initialization. WhE is it crucial to consider the variance of weights during initialization?\n",
    "\n",
    "### Ans.\n",
    "\n",
    "Weight initialization plays a crucial role in the training of artificial neural networks. It involves assigning initial values to the weights of the network before training begins. The choice of weight initialization method can greatly impact the model's performance, convergence speed, and generalization ability.\n",
    "\n",
    "Careful weight initialization is necessary in several scenarios:\n",
    "\n",
    "- Gradient-based optimization: In gradient-based optimization algorithms like backpropagation, the weights are updated iteratively based on the gradient of the loss function with respect to the weights. If the weights are initialized improperly, it can lead to undesirable outcomes like slow convergence or getting stuck in suboptimal solutions.\n",
    "\n",
    "- Deep neural networks: Deep neural networks with many layers are particularly sensitive to weight initialization. The gradients can vanish or explode as they propagate through the network during backpropagation. Proper initialization helps to alleviate these issues and ensure stable gradient flow.\n",
    "\n",
    "Improper weight initialization can lead to several challenges during training:\n",
    "\n",
    "- Vanishing and exploding gradients: When the weights are initialized too small or too large, the gradients can become extremely small or large as they propagate through the layers. This can cause the gradients to vanish or explode, making it difficult for the model to learn effectively.\n",
    "\n",
    "- Slow convergence: If the weights are initialized randomly but with large values, the initial predictions of the model can be far from the target values. As a result, the gradients can be large, leading to drastic weight updates. This can cause the training process to be unstable and slow down convergence.\n",
    "\n",
    "- Stuck in local minima: Improper weight initialization can increase the likelihood of the model getting trapped in poor local minima during optimization. This hinders the model's ability to find the global minimum and achieve optimal performance.\n",
    "\n",
    "The concept of variance is closely related to weight initialization. Variance refers to the measure of dispersion or spread of a distribution. During weight initialization, it is important to consider the variance of the weight values assigned to each neuron. If the variance is too high, it can lead to large activations and gradients, resulting in unstable training. On the other hand, if the variance is too low, the activations may become too small, making it difficult for the network to learn effectively.\n",
    "\n",
    "It is crucial to consider the variance of weights during initialization, especially in deep neural networks. The variances should be carefully chosen to balance the signal flow and gradient stability throughout the network. Various weight initialization techniques, such as Xavier initialization or He initialization, aim to set appropriate variances based on the number of input and output connections of each layer. By considering the variance, we can ensure a smooth and stable training process, leading to improved convergence and overall model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fad03c-0a8d-46c6-8eeb-5b6a1542080c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6728cf34-e2e9-4532-903f-f9031a0f8134",
   "metadata": {},
   "source": [
    "## Task-2 Weight Initializing Technique\n",
    "\n",
    "4. Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate to use.\n",
    "\n",
    "5. Describe the process of random initialization. How can random initialization be adjusted to mitigate potential issues like saturation or vanishing/exploding gradients?\n",
    "\n",
    "6. Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper weight initialization and the underlying theory behind it.\n",
    "\n",
    "7. Explain the concept of He initialization. How does it differ from Xavier's initialization, and when is it preferred.\n",
    "\n",
    "### Ans.\n",
    "\n",
    "1. Zero initialization:\n",
    "\n",
    "Zero initialization refers to the practice of setting all the weights in a neural network to zero. While it seems intuitive to start with all weights as equal, zero initialization has limitations. When all weights are initialized to zero, all neurons in a layer will have the same output, resulting in symmetric weight updates during backpropagation. As a result, all neurons will continue to receive the same gradients and update their weights symmetrically. This leads to the \"symmetry problem\" where neurons fail to break symmetry and learn distinct features, hindering the expressiveness and learning capacity of the network. Due to these limitations, zero initialization is generally not recommended for most neural network architectures.\n",
    "\n",
    "2. Random initialization:\n",
    "\n",
    "Random initialization involves assigning random values to the weights of a neural network. This helps break symmetry and allows each neuron to learn different features. Random initialization can be adjusted to mitigate potential issues like saturation or vanishing/exploding gradients. A common practice is to initialize the weights from a Gaussian distribution with zero mean and a small variance, such as 0.01. This small variance prevents extreme weight values that could lead to saturation or gradient explosion. Additionally, techniques like \"fan-in\" or \"fan-out\" scaling can be applied to adjust the variance based on the number of input and output connections of a layer, ensuring a balanced initialization.\n",
    "\n",
    "3. Xavier/Glorot initialization:\n",
    "\n",
    "Xavier/Glorot initialization, proposed by Xavier Glorot and Yoshua Bengio, is a widely used weight initialization technique. It aims to address the challenges of improper weight initialization and enable effective training of neural networks. The key idea behind Xavier initialization is to set the variance of the weights based on the number of input and output connections of a layer. The weights are sampled from a Gaussian distribution with zero mean and a variance calculated as:\n",
    "\n",
    "variance = 2 / (fan_in + fan_out)\n",
    "\n",
    "Here, fan_in is the number of input connections to a neuron, and fan_out is the number of output connections. By considering the fan_in and fan_out, Xavier initialization ensures that the signal is neither amplified nor diminished as it propagates through the network. This helps alleviate the issues of vanishing and exploding gradients, leading to improved training stability and convergence.\n",
    "\n",
    "4. He initialization:\n",
    "\n",
    "He initialization, named after its proposer Kaiming He, is another popular weight initialization method, particularly suited for networks with the Rectified Linear Unit (ReLU) activation function. It addresses a limitation of Xavier initialization, which assumes a linear activation function. He initialization adjusts the variance based only on the number of input connections (fan_in) and sets it as:\n",
    "variance = 2 / fan_in\n",
    "\n",
    "Since the ReLU activation function tends to suppress half of its input, He initialization compensates for this behavior by doubling the variance. This helps maintain the signal strength and mitigate the issue of vanishing gradients. He initialization is generally preferred for deep neural networks with ReLU activations, as it has shown to provide better performance compared to Xavier initialization in these scenarios.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e70dc3-5c6c-4dc1-9587-0d858db6e569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8a8c30f-dfe4-4c54-bfcf-be674847d32a",
   "metadata": {},
   "source": [
    "## Task-3 Applying Weight Initialization\n",
    "\n",
    "8. Implement different weight initialization techniques (zero initialization, random initialization, Xavier initialization, and He initialization) in a neural network using a framework of your choice. Train the model on a suitable dataset and compare the performance of the initialized models.\n",
    "\n",
    "9. Discuss the considerations and tradeoffs when choosing the appropriate weight initialization technique for a given neural network architecture and task.\n",
    "\n",
    "### Ans.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "298ba6ee-ef8b-4336-bc57-719123800352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.12.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.12)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.54.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.9.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c4a380-5f67-46fd-ba25-eba9e859018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 10:39:46.014559: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-13 10:39:46.091613: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-13 10:39:46.093104: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-13 10:39:47.346157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 2.3021 - accuracy: 0.1119 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 2.3021 - accuracy: 0.1098 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3010 - accuracy: 0.1124 - val_loss: 2.3006 - val_accuracy: 0.1135\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3006 - accuracy: 0.1124 - val_loss: 2.3002 - val_accuracy: 0.1135\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.3001 - accuracy: 0.1124 - val_loss: 2.2995 - val_accuracy: 0.1135\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.2989 - accuracy: 0.1124 - val_loss: 2.2975 - val_accuracy: 0.1135\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.2950 - accuracy: 0.1135 - val_loss: 2.2898 - val_accuracy: 0.1366\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.2683 - accuracy: 0.1958 - val_loss: 2.2163 - val_accuracy: 0.2099\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.0986 - accuracy: 0.2186 - val_loss: 1.9566 - val_accuracy: 0.2629\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 1.7980 - accuracy: 0.3418 - val_loss: 1.6487 - val_accuracy: 0.3958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.4287 - accuracy: 0.6206 - val_loss: 0.7285 - val_accuracy: 0.8189\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.5779 - accuracy: 0.8478 - val_loss: 0.4557 - val_accuracy: 0.8785\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.4276 - accuracy: 0.8821 - val_loss: 0.3765 - val_accuracy: 0.8942\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3709 - accuracy: 0.8957 - val_loss: 0.3375 - val_accuracy: 0.9039\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3397 - accuracy: 0.9031 - val_loss: 0.3132 - val_accuracy: 0.9115\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3185 - accuracy: 0.9092 - val_loss: 0.2965 - val_accuracy: 0.9154\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3022 - accuracy: 0.9139 - val_loss: 0.2839 - val_accuracy: 0.9213\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.2889 - accuracy: 0.9167 - val_loss: 0.2733 - val_accuracy: 0.9228\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.2771 - accuracy: 0.9207 - val_loss: 0.2625 - val_accuracy: 0.9259\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.2667 - accuracy: 0.9237 - val_loss: 0.2540 - val_accuracy: 0.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer HeUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.1462 - accuracy: 0.6865 - val_loss: 0.5607 - val_accuracy: 0.8581\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.4716 - accuracy: 0.8737 - val_loss: 0.3853 - val_accuracy: 0.8971\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3671 - accuracy: 0.8983 - val_loss: 0.3239 - val_accuracy: 0.9113\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3210 - accuracy: 0.9103 - val_loss: 0.2913 - val_accuracy: 0.9185\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.2929 - accuracy: 0.9172 - val_loss: 0.2700 - val_accuracy: 0.9231\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.2733 - accuracy: 0.9216 - val_loss: 0.2544 - val_accuracy: 0.9286\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.2577 - accuracy: 0.9264 - val_loss: 0.2415 - val_accuracy: 0.9308\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.2450 - accuracy: 0.9293 - val_loss: 0.2315 - val_accuracy: 0.9337\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.2339 - accuracy: 0.9326 - val_loss: 0.2211 - val_accuracy: 0.9358\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.2243 - accuracy: 0.9354 - val_loss: 0.2155 - val_accuracy: 0.9375\n",
      "Model initialized with Zeros\n",
      "Training accuracy: 0.11236666887998581\n",
      "Validation accuracy: 0.11349999904632568\n",
      "\n",
      "Model initialized with RandomNormal\n",
      "Training accuracy: 0.341783344745636\n",
      "Validation accuracy: 0.39579999446868896\n",
      "\n",
      "Model initialized with GlorotUniform\n",
      "Training accuracy: 0.9237499833106995\n",
      "Validation accuracy: 0.9294000267982483\n",
      "\n",
      "Model initialized with HeUniform\n",
      "Training accuracy: 0.9354000091552734\n",
      "Validation accuracy: 0.9375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import Zeros, RandomNormal, GlorotUniform, HeUniform\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28*28) / 255.0\n",
    "x_test = x_test.reshape(-1, 28*28) / 255.0\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Define the neural network architecture\n",
    "def create_model(initializer):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(784,), kernel_initializer=initializer))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer=initializer))\n",
    "    model.add(Dense(10, activation='softmax', kernel_initializer=initializer))\n",
    "    return model\n",
    "\n",
    "# Train and evaluate models with different weight initialization techniques\n",
    "initializers = [Zeros(), RandomNormal(stddev=0.01), GlorotUniform(), HeUniform()]\n",
    "histories = []\n",
    "\n",
    "for initializer in initializers:\n",
    "    model = create_model(initializer)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.01), metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))\n",
    "    histories.append(history)\n",
    "\n",
    "# Compare the performance of the models\n",
    "for i, history in enumerate(histories):\n",
    "    print(f\"Model initialized with {initializers[i].__class__.__name__}\")\n",
    "    print(\"Training accuracy:\", history.history['accuracy'][-1])\n",
    "    print(\"Validation accuracy:\", history.history['val_accuracy'][-1])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603fd98a-66ad-47bd-94f3-9633a239503e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4737d0a8-0081-432c-91d0-cdb0b3bb9de2",
   "metadata": {},
   "source": [
    "In this code snippet, we import the required libraries, load the MNIST dataset, and preprocess the data. Then, we define the neural network architecture using the Sequential API from TensorFlow and initialize the weights of each layer using different initialization techniques (Zeros, RandomNormal, GlorotUniform, and HeUniform). We compile the model with the specified loss function and optimizer and train the model for 10 epochs.\n",
    "\n",
    "After training, we evaluate the models and compare their performance based on the final training and validation accuracies.\n",
    "\n",
    "When choosing the appropriate weight initialization technique for a neural network, several considerations and tradeoffs come into play:\n",
    "\n",
    "- Activation functions: Different weight initialization techniques may work better with specific activation functions. For example, Xavier initialization is well-suited for activation functions like sigmoid or tanh, while He initialization is effective with ReLU-like activations. Consider the activation functions used in the network and choose an initialization technique accordingly.\n",
    "\n",
    "- Network depth: The depth of the network can impact the choice of weight initialization. Deeper networks are more prone to vanishing or exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15744e-0d5f-4293-8b67-4c5f24667b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

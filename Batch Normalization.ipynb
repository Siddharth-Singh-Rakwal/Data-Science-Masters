{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9caa923a-8e98-4e2e-8155-070a954385f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment | Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4732bb-6592-4c9a-812e-f50d3b7a3b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1a21e54-30f2-46a0-9246-11f3eb0d1b07",
   "metadata": {},
   "source": [
    "Objective: The objective of this assignment is to assess students' understanding of batch normalization in\n",
    "artificial neural networks (ANN) and its impact on training performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188fb54a-e9ab-4ba8-a7fa-e89eea1e3297",
   "metadata": {},
   "source": [
    "### Q1. Theory and Concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaef3c9-6227-4bb3-b0f7-21475452f990",
   "metadata": {},
   "source": [
    "1. Explain the concept of batch normalization in the context of Artificial Neural Networksr\n",
    "\n",
    "Ans.\n",
    "\n",
    "Batch normalization is a technique used in artificial neural networks to improve the training process and the performance of the model. It aims to address the problem of internal covariate shift, which refers to the change in the distribution of intermediate activations of the network layers during training.\n",
    "\n",
    "In a neural network, each layer receives inputs from the previous layer and applies a transformation, typically followed by an activation function. As the network trains, the distribution of inputs to each layer changes because the parameters of the preceding layers are being updated. This causes the network to constantly adapt to new input distributions, making the training process slower and less stable.\n",
    "\n",
    "Batch normalization helps alleviate this problem by normalizing the inputs to a layer across a mini-batch of training examples. The normalization is performed by subtracting the mini-batch mean and dividing by the mini-batch standard deviation. This step brings the inputs to zero mean and unit variance, effectively stabilizing the distribution of inputs for each layer.\n",
    "\n",
    "The batch normalization operation can be mathematically defined as follows:\n",
    "\n",
    "Given a mini-batch of inputs X = {x_1, x_2, ..., x_m} for a layer, the mean and variance are computed as:\n",
    "\n",
    "μ = (1/m) * Σ(x_i)\n",
    "\n",
    "σ^2 = (1/m) * Σ((x_i - μ)^2)\n",
    "\n",
    "Then, the inputs are normalized as:\n",
    "\n",
    "x_hat_i = (x_i - μ) / √(σ^2 + ε)\n",
    "\n",
    "Where ε is a small constant added for numerical stability to avoid division by zero.\n",
    "\n",
    "After normalization, the inputs are further transformed using two additional learnable parameters, scale (γ) and shift (β):\n",
    "\n",
    "y_i = γ * x_hat_i + β\n",
    "\n",
    "These scale and shift parameters are learned during the training process and allow the network to adjust the normalized inputs according to its needs.\n",
    "\n",
    "Batch normalization brings several benefits to the training process. Firstly, it reduces the internal covariate shift, making the optimization process more stable and accelerating convergence. It also acts as a form of regularization, reducing the dependence on specific initialization values and reducing the need for other regularization techniques like dropout. Additionally, it helps to smooth out the loss landscape, making it less likely to get stuck in poor local optima.\n",
    "\n",
    "Overall, batch normalization is an effective technique for improving the training speed and performance of neural networks by normalizing the inputs and stabilizing the distribution of intermediate activations within the network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f4a38-34fa-41c7-bb3d-f78759088ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3406223e-da3e-4853-97e9-b8982f5c2dc8",
   "metadata": {},
   "source": [
    "2. Describe the benefits of using batch normalization during trainingr\n",
    "\n",
    "Ans.\n",
    "\n",
    "Batch normalization offers several benefits when used during training in neural networks:\n",
    "\n",
    "- Stabilizes training: By normalizing the inputs to each layer, batch normalization reduces the problem of internal covariate shift. This stabilization helps in training the network more efficiently by mitigating the constant changes in input distribution that occur as the network parameters are updated.\n",
    "\n",
    "- Faster convergence: With a more stable training process, batch normalization can accelerate convergence. The normalization step ensures that the gradients flow more smoothly through the network, allowing for faster learning and convergence to optimal weights.\n",
    "\n",
    "- Enables higher learning rates: Normalizing the inputs with batch normalization allows for the use of higher learning rates. Higher learning rates can speed up the learning process and help the network reach good performance faster. Without batch normalization, using high learning rates may lead to unstable training or divergence.\n",
    "\n",
    "- Reduces sensitivity to weight initialization: Batch normalization reduces the dependence on weight initialization. It helps in mitigating the effect of poor initial parameter values, allowing the network to converge more reliably and effectively.\n",
    "\n",
    "- Acts as a regularizer: Batch normalization introduces a slight regularization effect during training. By normalizing the inputs across mini-batches, it reduces the reliance on specific instances or patterns in the training data. This can help prevent overfitting and improve the generalization capability of the model.\n",
    "\n",
    "- Improves gradient flow: Batch normalization reduces the magnitude of the gradients by normalizing the inputs. This addresses the vanishing or exploding gradient problem, making it easier for the gradients to propagate through the network. As a result, deeper networks can be trained more effectively.\n",
    "\n",
    "- Reduces the need for other regularization techniques: Batch normalization has a regularizing effect on its own, which can reduce the need for other regularization techniques such as dropout or weight decay. This simplifies the training process and may lead to better overall performance.\n",
    "\n",
    "- Enhances robustness to different input distributions: Batch normalization makes the network less sensitive to variations in the input distribution. This can be beneficial when the test data comes from a different distribution than the training data, as the normalization helps the network adapt to different data characteristics.\n",
    "\n",
    "Overall, batch normalization is a powerful technique that improves the training process in neural networks by stabilizing the training, accelerating convergence, reducing sensitivity to weight initialization, acting as a regularizer, improving gradient flow, and enhancing robustness to different input distributions.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f0962-43d1-4af5-8a29-95ee33797ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d28c788-77d7-428e-a651-612c9799c028",
   "metadata": {},
   "source": [
    "3. Discuss the working principle of batch normalization, including the normalization step and the learnable\n",
    "parameters.\n",
    "\n",
    "Ans.\n",
    "\n",
    "The working principle of batch normalization involves two main steps: normalization and the application of learnable parameters.\n",
    "\n",
    "1. Normalization Step: In batch normalization, the inputs to each layer are normalized across a mini-batch of training examples. The normalization is performed to bring the inputs to zero mean and unit variance. This step helps stabilize the distribution of inputs and reduces the internal covariate shift.\n",
    "\n",
    "The normalization process can be summarized as follows:\n",
    "\n",
    "- Given a mini-batch of inputs X = {x_1, x_2, ..., x_m} for a layer, where m is the mini-batch size.\n",
    "- Compute the mini-batch mean and variance:\n",
    "- Mean: μ = (1/m) * Σ(x_i)\n",
    "- Variance: σ^2 = (1/m) * Σ((x_i - μ)^2)\n",
    "- Normalize the inputs:\n",
    "- x_hat_i = (x_i - μ) / √(σ^2 + ε)\n",
    "\n",
    "Here, μ represents the mean of the mini-batch, σ^2 represents the variance, and ε is a small constant added for numerical stability to avoid division by zero.\n",
    "\n",
    "2. Learnable Parameters: After the normalization step, the normalized inputs are transformed using two additional learnable parameters: scale (γ) and shift (β). These parameters are learned during the training process and allow the network to adjust the normalized inputs according to its needs.\n",
    "\n",
    "The transformation of the normalized inputs can be expressed as:\n",
    "\n",
    "- y_i = γ * x_hat_i + β\n",
    "\n",
    "Here, γ represents the scaling factor, and β represents the shift factor. These parameters are initialized randomly and updated through backpropagation during training, just like any other network parameters. The scale parameter controls the amplitude of the normalized inputs, while the shift parameter controls the bias.\n",
    "\n",
    "The introduction of these learnable parameters allows the network to learn the optimal scaling and shifting of the normalized inputs for each layer. This flexibility helps the network adapt and exploit the strengths of different activation ranges.\n",
    "\n",
    "During inference or testing, batch normalization operates slightly differently. Instead of computing the mean and variance across a mini-batch, it uses the population statistics computed during training. This ensures consistency in the normalization process and allows for better generalization to unseen data.\n",
    "\n",
    "By incorporating the normalization step and the learnable parameters, batch normalization brings the inputs to each layer to a more stable and normalized range, improving the training process and the overall performance of neural networks.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa68e6-c34b-4ff6-8334-6e91a19c211b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2d47957-c786-4da9-807d-e10c0bdc3a34",
   "metadata": {},
   "source": [
    "## Q2. Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d622f350-68d2-45eb-9fe9-7b5b3147bf3d",
   "metadata": {},
   "source": [
    "1. Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess it.\n",
    "2. Implement a simple feedforward neural network using any deep learning framework/library (e.g.,Tensorlow, PyTorch)\n",
    "3. Train the neural network on the chosen dataset without using batch normalization\n",
    "4. Implement batch normalization layers in the neural network and train the model again\n",
    "5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization\n",
    "6. Discuss the impact of batch normalization on the training process and the performance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835861a6-c162-434a-965d-c5946c430107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.5.post0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90d7971-7dbe-48d7-b444-632b111a30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 171821049.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 48727149.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 282028031.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 8683012.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Accuracy: 0.9439, Validation Accuracy: 0.9414\n",
      "Epoch [2/10], Train Accuracy: 0.9549, Validation Accuracy: 0.9500\n",
      "Epoch [3/10], Train Accuracy: 0.9702, Validation Accuracy: 0.9668\n",
      "Epoch [4/10], Train Accuracy: 0.9768, Validation Accuracy: 0.9697\n",
      "Epoch [5/10], Train Accuracy: 0.9771, Validation Accuracy: 0.9669\n",
      "Epoch [6/10], Train Accuracy: 0.9769, Validation Accuracy: 0.9678\n",
      "Epoch [7/10], Train Accuracy: 0.9840, Validation Accuracy: 0.9758\n",
      "Epoch [8/10], Train Accuracy: 0.9840, Validation Accuracy: 0.9740\n",
      "Epoch [9/10], Train Accuracy: 0.9845, Validation Accuracy: 0.9735\n",
      "Epoch [10/10], Train Accuracy: 0.9849, Validation Accuracy: 0.9748\n",
      "Epoch [1/10], Train Accuracy: 0.9676, Validation Accuracy: 0.9652\n",
      "Epoch [2/10], Train Accuracy: 0.9698, Validation Accuracy: 0.9629\n",
      "Epoch [3/10], Train Accuracy: 0.9767, Validation Accuracy: 0.9685\n",
      "Epoch [4/10], Train Accuracy: 0.9788, Validation Accuracy: 0.9697\n",
      "Epoch [5/10], Train Accuracy: 0.9835, Validation Accuracy: 0.9753\n",
      "Epoch [6/10], Train Accuracy: 0.9851, Validation Accuracy: 0.9762\n",
      "Epoch [7/10], Train Accuracy: 0.9857, Validation Accuracy: 0.9753\n",
      "Epoch [8/10], Train Accuracy: 0.9852, Validation Accuracy: 0.9755\n",
      "Epoch [9/10], Train Accuracy: 0.9883, Validation Accuracy: 0.9779\n",
      "Epoch [10/10], Train Accuracy: 0.9879, Validation Accuracy: 0.9771\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Preprocessing\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, use_batch_norm=False):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "        if self.use_batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(256)\n",
    "            self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        \n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        \n",
    "        x = self.relu(self.fc2(x))\n",
    "        \n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Train the model without batch normalization\n",
    "def train(model, criterion, optimizer, dataloader):\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = total_correct / len(dataloader.dataset)\n",
    "    return accuracy\n",
    "\n",
    "# Training without batch normalization\n",
    "model_no_bn = FeedForwardNet(use_batch_norm=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_no_bn.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train(model_no_bn, criterion, optimizer, train_loader)\n",
    "    train_accuracy = evaluate(model_no_bn, train_loader)\n",
    "    val_accuracy = evaluate(model_no_bn, test_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Training with batch normalization\n",
    "model_with_bn = FeedForwardNet(use_batch_norm=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_with_bn.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train(model_with_bn, criterion, optimizer, train_loader)\n",
    "    train_accuracy = evaluate(model_with_bn, train_loader)\n",
    "    val_accuracy = evaluate(model_with_bn, test_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389d7cf-c677-446c-a137-9b5dc6f13999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "474bc8d8-e456-4569-bd07-46b29fda96cc",
   "metadata": {},
   "source": [
    "In this code, we first preprocess the MNIST dataset using PyTorch's DataLoader. Then, we define a simple feedforward neural network called FeedForwardNet with and without batch normalization. We train and evaluate the models using the training and test datasets, respectively. Finally, we compare the training and validation accuracies of the models trained with and without batch normalization for each epoch.\n",
    "\n",
    "Note that the code provided is a basic example, and you can modify it further to suit your specific needs or experiment with different network architectures, optimizers, or hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8755a03-4e4b-494d-9f47-b549ad0f7a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dffd6f4-f74f-4f01-802d-735f404187c2",
   "metadata": {},
   "source": [
    "## Q3. Experimentation and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd79ea-02d6-4e95-8f3b-0576e4222f0d",
   "metadata": {},
   "source": [
    "1. Experiment with different batch sizes and observe the effect on the training dynamics and model performance.\n",
    "2. Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fdad506-eaa9-4f6b-819b-7c37c126102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 32, Epoch [1/10], Train Accuracy: 0.9475, Validation Accuracy: 0.9454\n",
      "Batch Size: 32, Epoch [2/10], Train Accuracy: 0.9608, Validation Accuracy: 0.9589\n",
      "Batch Size: 32, Epoch [3/10], Train Accuracy: 0.9682, Validation Accuracy: 0.9645\n",
      "Batch Size: 32, Epoch [4/10], Train Accuracy: 0.9732, Validation Accuracy: 0.9668\n",
      "Batch Size: 32, Epoch [5/10], Train Accuracy: 0.9770, Validation Accuracy: 0.9707\n",
      "Batch Size: 32, Epoch [6/10], Train Accuracy: 0.9761, Validation Accuracy: 0.9706\n",
      "Batch Size: 32, Epoch [7/10], Train Accuracy: 0.9797, Validation Accuracy: 0.9696\n",
      "Batch Size: 32, Epoch [8/10], Train Accuracy: 0.9822, Validation Accuracy: 0.9734\n",
      "Batch Size: 32, Epoch [9/10], Train Accuracy: 0.9812, Validation Accuracy: 0.9713\n",
      "Batch Size: 32, Epoch [10/10], Train Accuracy: 0.9845, Validation Accuracy: 0.9739\n",
      "Batch Size: 64, Epoch [1/10], Train Accuracy: 0.9443, Validation Accuracy: 0.9428\n",
      "Batch Size: 64, Epoch [2/10], Train Accuracy: 0.9600, Validation Accuracy: 0.9588\n",
      "Batch Size: 64, Epoch [3/10], Train Accuracy: 0.9675, Validation Accuracy: 0.9610\n",
      "Batch Size: 64, Epoch [4/10], Train Accuracy: 0.9734, Validation Accuracy: 0.9676\n",
      "Batch Size: 64, Epoch [5/10], Train Accuracy: 0.9762, Validation Accuracy: 0.9722\n",
      "Batch Size: 64, Epoch [6/10], Train Accuracy: 0.9788, Validation Accuracy: 0.9702\n",
      "Batch Size: 64, Epoch [7/10], Train Accuracy: 0.9819, Validation Accuracy: 0.9721\n",
      "Batch Size: 64, Epoch [8/10], Train Accuracy: 0.9832, Validation Accuracy: 0.9730\n",
      "Batch Size: 64, Epoch [9/10], Train Accuracy: 0.9838, Validation Accuracy: 0.9733\n",
      "Batch Size: 64, Epoch [10/10], Train Accuracy: 0.9869, Validation Accuracy: 0.9757\n",
      "Batch Size: 128, Epoch [1/10], Train Accuracy: 0.9370, Validation Accuracy: 0.9356\n",
      "Batch Size: 128, Epoch [2/10], Train Accuracy: 0.9522, Validation Accuracy: 0.9479\n",
      "Batch Size: 128, Epoch [3/10], Train Accuracy: 0.9645, Validation Accuracy: 0.9581\n",
      "Batch Size: 128, Epoch [4/10], Train Accuracy: 0.9639, Validation Accuracy: 0.9591\n",
      "Batch Size: 128, Epoch [5/10], Train Accuracy: 0.9732, Validation Accuracy: 0.9650\n",
      "Batch Size: 128, Epoch [6/10], Train Accuracy: 0.9777, Validation Accuracy: 0.9696\n",
      "Batch Size: 128, Epoch [7/10], Train Accuracy: 0.9779, Validation Accuracy: 0.9689\n",
      "Batch Size: 128, Epoch [8/10], Train Accuracy: 0.9824, Validation Accuracy: 0.9709\n",
      "Batch Size: 128, Epoch [9/10], Train Accuracy: 0.9789, Validation Accuracy: 0.9696\n",
      "Batch Size: 128, Epoch [10/10], Train Accuracy: 0.9835, Validation Accuracy: 0.9729\n",
      "Batch Size: 256, Epoch [1/10], Train Accuracy: 0.9233, Validation Accuracy: 0.9221\n",
      "Batch Size: 256, Epoch [2/10], Train Accuracy: 0.9450, Validation Accuracy: 0.9402\n",
      "Batch Size: 256, Epoch [3/10], Train Accuracy: 0.9554, Validation Accuracy: 0.9506\n",
      "Batch Size: 256, Epoch [4/10], Train Accuracy: 0.9603, Validation Accuracy: 0.9551\n",
      "Batch Size: 256, Epoch [5/10], Train Accuracy: 0.9686, Validation Accuracy: 0.9627\n",
      "Batch Size: 256, Epoch [6/10], Train Accuracy: 0.9695, Validation Accuracy: 0.9631\n",
      "Batch Size: 256, Epoch [7/10], Train Accuracy: 0.9759, Validation Accuracy: 0.9684\n",
      "Batch Size: 256, Epoch [8/10], Train Accuracy: 0.9781, Validation Accuracy: 0.9688\n",
      "Batch Size: 256, Epoch [9/10], Train Accuracy: 0.9777, Validation Accuracy: 0.9661\n",
      "Batch Size: 256, Epoch [10/10], Train Accuracy: 0.9802, Validation Accuracy: 0.9702\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Preprocessing\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define the neural network architecture\n",
    "    class FeedForwardNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(FeedForwardNet, self).__init__()\n",
    "            self.fc1 = nn.Linear(784, 256)\n",
    "            self.fc2 = nn.Linear(256, 128)\n",
    "            self.fc3 = nn.Linear(128, 10)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "\n",
    "    # Training without batch normalization\n",
    "    model = FeedForwardNet()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        train_accuracy = evaluate(model, train_loader)\n",
    "        val_accuracy = evaluate(model, test_loader)\n",
    "        print(f\"Batch Size: {batch_size}, Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct / len(dataloader.dataset)\n",
    "    return accuracy\n",
    "\n",
    "# Experiment with different batch sizes\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    train_and_evaluate(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd441d-8184-4ce2-b473-b7b9c5a1076e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f3be935-973b-47f7-a889-d312bb04c23c",
   "metadata": {},
   "source": [
    "In this code, we define the train_and_evaluate function that takes a batch size as an input. Inside this function, we create data loaders with the specified batch size, and then train and evaluate the model using the provided batch size. We experiment with different batch sizes by iterating through the batch_sizes list and calling train_and_evaluate for each batch size.\n",
    "\n",
    "By running this code, you can observe the effect of different batch sizes on the training dynamics and model performance. You can analyze the training and validation accuracy for each batch size and compare the results. Note that larger batch sizes may result in faster training due to more efficient parallel computations, but they can also lead to slower convergence or poorer generalization. Smaller batch sizes may provide more noisy updates but can result in faster convergence or better generalization.\n",
    "\n",
    "The advantages of batch normalization in improving the training of neural networks include:\n",
    "\n",
    "- Stabilizing training: Batch normalization helps stabilize the training process by reducing the internal covariate shift and ensuring consistent input distributions throughout the network, which can result in faster convergence.\n",
    "- Regularization: Batch normalization acts as a form of regularization by adding noise to the network during training, which can help prevent overfitting and improve generalization performance.\n",
    "- Allowing higher learning rates: Batch normalization reduces the dependence of the network on the scale of the initial weights, allowing for the use of higher learning rates without the risk of unstable or divergent training.\n",
    "- Reducing the sensitivity to initialization: Batch normalization reduces the sensitivity of the network to the choice of weight initialization, making it easier to initialize and train deep networks.\n",
    "\n",
    "However, there are potential limitations of batch normalization to consider:\n",
    "\n",
    "1. Batch size dependence: Batch normalization performs normalization based on the statistics of the mini-batch, which means that the performance can be sensitive to the batch size used during training. Very small batch sizes may result in inaccurate statistics and degrade performance.\n",
    "2. Inference-time behavior: During inference, the network uses population statistics calculated during training rather than mini-batch statistics. This can introduce some discrepancy between training and inference, particularly when the batch size used during training is small.\n",
    "3. Added computational overhead: Batch normalization introduces additional computations, which may slightly increase training time. However, modern hardware and optimized implementations have mitigated this overhead to a large extent.\n",
    "\n",
    "These advantages and limitations should be considered when deciding whether to use batch normalization in a particular neural network architecture and training scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6c855-74ee-4b91-ade3-413e82e18918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

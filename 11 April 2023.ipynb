{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65e40a4-f5fe-4f87-b615-baee822bd083",
   "metadata": {},
   "source": [
    "# Assignment | 11th April 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de165f81-a791-42d7-bab6-edea9e3487a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b62700e-fd99-4de3-95fb-4b60c9c76c20",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "\n",
    "Ans.\n",
    "\n",
    "An ensemble technique in machine learning refers to the process of combining multiple individual models, called base learners, to improve the overall predictive performance of a system. The idea behind ensemble methods is that by combining the predictions of several models, the resulting ensemble can often achieve better results than any single model on its own.\n",
    "\n",
    "There are different types of ensemble techniques, including:\n",
    "\n",
    "1. Bagging (Bootstrap Aggregating): This method involves training multiple models on different subsets of the training data, obtained through bootstrapping (sampling with replacement). The predictions of the individual models are then combined, typically by averaging or majority voting, to make the final prediction. Random Forest is an example of a bagging ensemble algorithm.\n",
    "\n",
    "2. Boosting: Boosting is an iterative ensemble technique that trains a series of weak models sequentially, with each subsequent model trying to correct the mistakes made by the previous ones. The final prediction is made by aggregating the predictions of all the weak models, usually by weighted voting. Examples of boosting algorithms include AdaBoost, Gradient Boosting Machines (GBM), and XGBoost.\n",
    "\n",
    "3. Stacking: Stacking involves training multiple base models on the training data and then training a meta-model, often called a blender or meta-learner, on the predictions of the base models. The base models act as the input to the meta-model, which learns how to combine their predictions effectively. Stacking allows the meta-model to learn higher-level patterns from the base models' predictions. It can be used with various types of models, such as decision trees, support vector machines (SVM), or neural networks.\n",
    "\n",
    "4. Voting: Voting methods combine the predictions of multiple models by either majority voting (for classification tasks) or averaging (for regression tasks). In majority voting, each model in the ensemble votes for a class, and the class with the majority of votes is selected as the final prediction. Voting can be done with equal weights for all models or with weights based on their performance or expertise.\n",
    "\n",
    "Ensemble techniques are widely used in machine learning because they can enhance model generalization, reduce overfitting, and improve the robustness and accuracy of predictions. They leverage the wisdom of the crowd, combining the strengths of different models to compensate for their individual weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71597130-90b4-4b08-a164-cc7ba63f02dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "268d765d-7922-478c-87e0-323fedbd1741",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Ensemble techniques are used in machine learning for several reasons:\n",
    "\n",
    "1. Improved Predictive Performance: Ensemble methods often yield better predictive performance compared to individual models. By combining the predictions of multiple models, the ensemble can capture a broader range of patterns and reduce the impact of errors or biases in individual models. This can lead to more accurate and robust predictions.\n",
    "\n",
    "2. Reduced Overfitting: Ensemble techniques can help reduce overfitting, which occurs when a model learns to fit the training data too closely and performs poorly on unseen data. By combining multiple models that have been trained on different subsets of the data or using different algorithms, ensembles can reduce the likelihood of overfitting and improve generalization to unseen data.\n",
    "\n",
    "3. Increased Robustness: Ensemble methods can enhance the robustness of predictions by reducing the impact of outliers or noisy data points. If a single model is sensitive to outliers, the ensemble can mitigate their effect by aggregating predictions from multiple models, which may not all be affected by the same outliers.\n",
    "\n",
    "4. Capturing Diverse Perspectives: Different models may have different strengths and weaknesses. By combining models that have been trained using different algorithms or with different hyperparameters, ensembles can capture diverse perspectives and leverage the strengths of each model. This allows the ensemble to perform well across a wider range of scenarios or data distributions.\n",
    "\n",
    "5. Handling Model Uncertainty: Ensemble techniques can provide a measure of uncertainty or confidence in the predictions. By examining the level of agreement or disagreement among the individual models in the ensemble, it is possible to estimate the uncertainty associated with a prediction. This can be particularly useful in critical decision-making scenarios or when dealing with imbalanced or ambiguous data.\n",
    "\n",
    "6. Flexibility and Adaptability: Ensemble methods can be applied to various types of models and data. They are not limited to specific algorithms or domains, which makes them versatile and adaptable to different problems. Ensemble techniques can be combined with any base model, ranging from decision trees and support vector machines to neural networks and deep learning models.\n",
    "\n",
    "Overall, ensemble techniques are used in machine learning to improve predictive performance, reduce overfitting, increase robustness, capture diverse perspectives, handle uncertainty, and enhance the flexibility of the modeling process. They are powerful tools for boosting model performance and are widely used in many real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234b747-6f76-475d-9c50-6fbf355e84e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6abc1ecd-0031-4384-9808-a8e9ba1ed173",
   "metadata": {},
   "source": [
    "Q3. What is bagging?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning that involves combining the predictions of multiple models trained on different subsets of the training data. The main idea behind bagging is to create multiple subsets of the original training data by sampling with replacement (bootstrapping) and train a separate model on each subset.\n",
    "\n",
    "Here are the key steps in the bagging process:\n",
    "\n",
    "- Bootstrap Sampling: Random samples of the training data are created by drawing instances from the original dataset with replacement. This means that some instances may be selected multiple times, while others may not be selected at all. Each bootstrap sample is of the same size as the original dataset.\n",
    "\n",
    "- Model Training: A separate base model is trained on each bootstrap sample. These models are typically constructed using the same learning algorithm or model architecture, such as decision trees or neural networks. Each model is trained independently and has no knowledge of the other models.\n",
    "\n",
    "- Prediction Aggregation: Once the individual models are trained, predictions are made on unseen data using each model. For classification tasks, the most common aggregation method is majority voting, where the class predicted by the majority of models is chosen as the final prediction. For regression tasks, the predictions from all models are averaged.\n",
    "\n",
    "Bagging can offer several advantages:\n",
    "\n",
    "- Reduced Variance: By training models on different bootstrap samples, bagging reduces the variance of the predictions. Since each model is trained on a subset of the data, they are exposed to different instances and may learn different patterns. By aggregating their predictions, the overall variance is decreased, leading to more stable and reliable predictions.\n",
    "\n",
    "- Improved Generalization: Bagging helps to mitigate overfitting, which occurs when a model becomes too specialized to the training data and performs poorly on new, unseen data. By using bootstrap samples and combining the predictions of multiple models, bagging encourages generalization and reduces the risk of overfitting.\n",
    "\n",
    "- Robustness to Outliers: Bagging can make predictions more robust to outliers or noisy data points. Since each model is trained on a different subset of the data, the impact of outliers is reduced. The ensemble's final prediction is based on the majority consensus of the models, which helps to mitigate the influence of individual outliers.\n",
    "\n",
    "- Parallelization: Bagging allows for easy parallelization of the training process. Since each model is trained independently on its bootstrap sample, the training of individual models can be parallelized, enabling faster model training and scalability.\n",
    "\n",
    "Random Forest is one of the most popular ensemble algorithms based on bagging. It uses decision trees as base models and aggregates their predictions to make final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d088b489-351a-4661-bdb6-affb1afe692f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f56e979f-186d-4b89-aa34-48d6b8430e55",
   "metadata": {},
   "source": [
    "Q4. What is boosting?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Boosting is an ensemble technique in machine learning that aims to sequentially train a series of weak models (also known as base learners) to create a strong model. Unlike bagging, which trains models independently, boosting trains models in a sequential manner, where each subsequent model is built to improve upon the mistakes made by the previous models. The core idea behind boosting is to combine the predictions of multiple weak models to create a powerful ensemble model.\n",
    "\n",
    "Here are the key steps involved in boosting:\n",
    "\n",
    "- Base Model Training: Initially, the first base model is trained on the training data. This base model can be any weak learning algorithm, which is typically a simple model with low predictive power, such as a decision stump (a one-level decision tree) or a shallow neural network.\n",
    "\n",
    "- Instance Weighting: After the initial model is trained, the instances in the training data are assigned weights. Initially, all instances are given equal weights. However, in subsequent iterations, the weights are adjusted based on the performance of the previous models. Instances that were misclassified or had higher errors in the previous iteration are assigned higher weights, which increases their importance in the next iteration.\n",
    "\n",
    "- Sequential Model Training: In each iteration, a new base model is trained on the weighted training data. The models are built to focus on the instances that were previously misclassified or had higher errors. By sequentially training multiple models, boosting aims to correct the mistakes made by earlier models and improve overall predictive performance.\n",
    "\n",
    "- Model Weighting: After each model is trained, it is assigned a weight based on its performance. Models that perform better in terms of reducing errors or misclassifications are assigned higher weights, indicating their stronger influence in the final prediction.\n",
    "\n",
    "- Prediction Aggregation: When making predictions on unseen data, the predictions of all the base models are combined using weighted voting. The weights assigned to the models during training are used to determine the contribution of each model to the final prediction. The weighted aggregation allows the ensemble model to give more importance to the predictions of better-performing models.\n",
    "\n",
    "Boosting offers several benefits:\n",
    "\n",
    "- Improved Predictive Accuracy: Boosting can lead to significantly improved predictive accuracy compared to individual weak models. By iteratively training models that focus on correcting the mistakes of previous models, boosting can effectively capture complex patterns and achieve high accuracy.\n",
    "\n",
    "- Handling Imbalanced Data: Boosting algorithms can handle imbalanced datasets well. As the models are trained sequentially, instances that were previously misclassified or had higher errors receive higher weights, which makes subsequent models pay more attention to them. This helps in giving more importance to minority class instances, improving the performance on imbalanced datasets.\n",
    "\n",
    "- Feature Importance: Boosting algorithms can provide information about feature importance. By tracking the weights assigned to features during training, boosting algorithms can identify the most relevant features for making accurate predictions. This information can be useful in feature selection and understanding the underlying patterns in the data.\n",
    "\n",
    "- Flexibility: Boosting is a versatile technique that can be applied to different types of weak models, such as decision trees, neural networks, or support vector machines. This flexibility allows boosting to be used in a wide range of machine learning tasks and domains.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting Machines (GBM), XGBoost (Extreme Gradient Boosting), and LightGBM (Light Gradient Boosting Machine). These algorithms differ in their specific implementation details, but they all follow the general concept of boosting by sequentially training models and adjusting instance weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788dabe-8ad0-48bd-9a12-d55779009193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01c33aa8-78df-4650-86fd-19adb8ca3990",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Using ensemble techniques in machine learning can provide several benefits, including:\n",
    "\n",
    "- Improved Predictive Performance: Ensemble methods often result in improved predictive performance compared to individual models. By combining multiple models, ensembles can capture a broader range of patterns and make more accurate predictions. The ensemble can mitigate the weaknesses and biases of individual models, leading to more robust and reliable predictions.\n",
    "\n",
    "- Reduced Overfitting: Ensemble techniques help reduce overfitting, which occurs when a model becomes too complex and overly specialized to the training data, resulting in poor generalization to unseen data. Ensembles, by combining models trained on different subsets of the data or using different algorithms, can reduce the risk of overfitting and improve generalization performance.\n",
    "\n",
    "- Increased Robustness: Ensembles are more robust to outliers or noisy data points. Individual models may be sensitive to outliers, but ensembles can mitigate their impact by combining predictions from multiple models. Outliers or errors in individual models are likely to have a smaller influence on the final ensemble prediction, making the overall prediction more reliable and robust.\n",
    "\n",
    "- Capturing Diverse Perspectives: Ensemble techniques allow for capturing diverse perspectives by combining models with different strengths and weaknesses. Different models may excel in different aspects of the data or have different biases. By combining their predictions, ensembles can leverage the strengths of each model and compensate for their individual weaknesses, resulting in a more comprehensive analysis of the data.\n",
    "\n",
    "- Handling Model Uncertainty: Ensemble methods can provide estimates of uncertainty or confidence in predictions. By examining the agreement or disagreement among the individual models in the ensemble, it is possible to quantify the uncertainty associated with a prediction. This information is valuable in decision-making scenarios where knowing the confidence level of predictions is important.\n",
    "\n",
    "- Flexibility and Adaptability: Ensemble techniques are flexible and adaptable to different types of models, algorithms, and data. They can be applied to various machine learning tasks, ranging from classification and regression to anomaly detection and recommendation systems. Ensembles can be constructed with different types of models, such as decision trees, neural networks, or support vector machines, allowing for versatility in modeling approaches.\n",
    "\n",
    "- Model Stability: Ensemble methods tend to improve the stability of the model. As ensembles are built by combining multiple models, the impact of individual model variations or fluctuations is reduced. This leads to more stable and consistent predictions, making ensembles more reliable in production systems.\n",
    "\n",
    "- Enabling Parallelization: Some ensemble techniques, such as bagging, allow for easy parallelization of model training. Since each model in the ensemble can be trained independently on different subsets of the data, the training process can be parallelized, resulting in faster model training and improved scalability.\n",
    "\n",
    "Overall, ensemble techniques offer a powerful approach to improve the accuracy, robustness, and generalization of machine learning models. They leverage the wisdom of multiple models, capturing diverse perspectives and mitigating individual weaknesses. Ensemble methods have been successfully applied in various domains and have become an integral part of many state-of-the-art machine learning systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eeafc4-5f98-453c-bcc1-9ff5fc462c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d55fb16-bf9d-4a9b-9404-0a58991f4aef",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Ensemble techniques are not always better than individual models in every scenario. While ensemble methods can provide improved predictive performance in many cases, there are situations where individual models might be more appropriate or sufficient. The effectiveness of ensemble techniques depends on several factors, including the specific problem, the quality of the individual models, the diversity of the models, and the availability of data.\n",
    "\n",
    "Here are some scenarios where ensemble techniques may not necessarily be better than individual models:\n",
    "\n",
    "- Limited Data: Ensemble techniques require a sufficient amount of diverse data to train multiple models effectively. If the available data is limited or insufficient to create diverse subsets for training the ensemble, individual models might perform better. In such cases, training a single model on the available data could yield better results.\n",
    "\n",
    "- High-Quality Individual Models: If you already have a single model that performs exceptionally well on its own and produces accurate predictions, it might not be necessary to use ensemble techniques. Ensemble methods are particularly beneficial when there are multiple weak models that can be combined to compensate for each other's limitations. However, if the individual model already achieves high accuracy, ensemble methods may not provide a significant improvement.\n",
    "\n",
    "- Computation and Resource Constraints: Ensemble techniques can be computationally intensive and require additional resources compared to individual models. If you have strict constraints on computation time, memory, or system resources, training and maintaining an ensemble might not be feasible or practical. In such cases, focusing on a single high-performing model might be more appropriate.\n",
    "\n",
    "- Interpretability Requirements: Ensemble methods typically result in more complex models compared to individual models. If interpretability is a crucial requirement, and understanding the reasoning behind predictions is important, an individual model, such as a decision tree, might be preferred over an ensemble. Individual models often provide more transparent and interpretable insights.\n",
    "\n",
    "- Training Time and Deployment Constraints: Ensemble methods generally require more time for training as multiple models need to be trained and combined. If there are strict time constraints for model training or deployment, it may not be feasible to use ensemble techniques. In such cases, training and deploying a single model can be more efficient.\n",
    "\n",
    "It's important to consider the specific context, requirements, and constraints of the problem at hand when deciding whether to use ensemble techniques or rely on individual models. Ensemble methods are valuable tools in machine learning, but their applicability and effectiveness depend on the specific circumstances and trade-offs involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfdf730-99e2-456b-a09a-9d4f900081c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9d57b38-9941-457a-b794-68df6b38e976",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "Ans.\n",
    "\n",
    "The confidence interval can be estimated using bootstrap resampling, a technique that involves sampling with replacement from the original dataset to create multiple bootstrap samples. Here's a general approach to calculating the confidence interval using bootstrap:\n",
    "\n",
    "- Data Preparation: Start with your original dataset of size N. Determine the desired confidence level, typically denoted as (1 - α), where α is the significance level (e.g., 0.05 for a 95% confidence level).\n",
    "\n",
    "- Bootstrap Sampling: Generate B bootstrap samples by randomly sampling N instances from the original dataset with replacement. Each bootstrap sample should have the same size as the original dataset.\n",
    "\n",
    "- Model Fitting and Estimation: For each bootstrap sample, train your model of interest and obtain the desired estimate or prediction. This could be, for example, the mean, median, standard deviation, or any other relevant statistic you want to estimate.\n",
    "\n",
    "- Estimate Calculation: Calculate the estimate of interest for each bootstrap sample, resulting in B estimate values.\n",
    "\n",
    "- Confidence Interval Construction: Using the B estimate values obtained from the previous step, calculate the lower and upper percentiles of the desired confidence interval. For a confidence level of (1 - α), the lower percentile would be α/2, and the upper percentile would be 1 - α/2.\n",
    "\n",
    "- Confidence Interval Calculation: The confidence interval is constructed by taking the lower and upper percentiles calculated in the previous step as the bounds of the interval. This interval provides an estimation of the range within which the true population parameter is likely to fall at the specified confidence level.\n",
    "\n",
    "It's important to note that the confidence interval calculated using bootstrap resampling is based on the assumption that the original dataset is representative of the population. Additionally, the accuracy and reliability of the confidence interval depend on the number of bootstrap samples (B). A larger number of bootstrap samples generally leads to more accurate confidence intervals, but it also increases computational requirements.\n",
    "\n",
    "Bootstrap resampling provides a non-parametric approach to estimating confidence intervals, making it a useful technique when assumptions about the underlying data distribution are unclear or violated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc441b29-0675-4c7e-aae9-9ea4f6085ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a58f35e0-81d4-44f8-b90b-e3041e7540a4",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic or to assess the uncertainty of an estimate by generating multiple samples from the original dataset. The bootstrap approach allows us to make inferences about the population without relying on assumptions about the underlying data distribution. Here are the steps involved in the bootstrap process:\n",
    "\n",
    "- Data Preparation: Start with the original dataset of size N.\n",
    "\n",
    "- Sample Generation: Generate a bootstrap sample by randomly sampling N instances from the original dataset with replacement. This means that each instance has an equal chance of being selected for the bootstrap sample, and some instances may be selected multiple times, while others may not be selected at all. The size of the bootstrap sample is the same as the original dataset.\n",
    "\n",
    "- Statistical Calculation: Perform the desired statistical calculation or analysis on the bootstrap sample. This could involve calculating a summary statistic, such as the mean, median, standard deviation, or any other relevant measure you want to estimate.\n",
    "\n",
    "- Repeat Steps 2-3: Repeat steps 2 and 3 multiple times, typically B times, to generate B bootstrap samples and compute the desired statistic for each sample.\n",
    "\n",
    "- Statistical Inference: Analyze the distribution of the statistics obtained from the B bootstrap samples. This distribution represents the empirical sampling distribution of the statistic.\n",
    "\n",
    "- Confidence Interval and Hypothesis Testing: Using the distribution of the bootstrap statistics, construct confidence intervals or perform hypothesis tests to make inferences about the population parameter of interest. For example, you can calculate the lower and upper percentiles of the bootstrap distribution to estimate a confidence interval, or compare the observed statistic from the original dataset to the bootstrap distribution to assess hypotheses.\n",
    "\n",
    "By generating multiple bootstrap samples and analyzing the distribution of statistics, bootstrap resampling provides an approximation of the sampling distribution of the statistic of interest. This allows for statistical inference and estimation without relying on assumptions about the underlying data distribution. Bootstrap can be applied to various statistical analyses and machine learning algorithms to assess uncertainty, validate models, estimate confidence intervals, and perform hypothesis testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd28b2b-f5d8-4aee-8048-9acae8420ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "961f427e-050c-4176-a518-f2f55accd9ac",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height.\n",
    "\n",
    "Ans.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5572820-150a-447a-be3a-e9efe11e84e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for Mean Height: (14.457712260345433, 15.560081893810205)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "sample_mean = 15  # mean height of the sample\n",
    "sample_std = 2  # standard deviation of the sample\n",
    "\n",
    "# Number of bootstrap samples\n",
    "B = 1000\n",
    "\n",
    "# Bootstrap resampling\n",
    "bootstrap_means = []\n",
    "for _ in range(B):\n",
    "    bootstrap_sample = np.random.normal(sample_mean, sample_std, size=50)\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Confidence interval calculation\n",
    "lower_percentile = np.percentile(bootstrap_means, 2.5)\n",
    "upper_percentile = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "# Confidence interval interpretation\n",
    "confidence_interval = (lower_percentile, upper_percentile)\n",
    "print(\"95% Confidence Interval for Mean Height:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc20f14-82ca-4075-ad52-8ce77142edea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff5a8a8-6ad6-4a5e-b8c4-2c13a80bba18",
   "metadata": {},
   "source": [
    "# Assignment | 17th April 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c9524-5d32-4a51-b769-63f32afa7916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01fb2a4b-43e8-4e68-8f8c-ff818f1b6a43",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Gradient Boosting Regression is a machine learning algorithm that combines the principles of gradient boosting and regression. It is a powerful and popular method for solving regression problems, where the goal is to predict a continuous numeric value.\n",
    "\n",
    "In gradient boosting regression, an ensemble of weak regression models, typically decision trees, is sequentially trained to correct the errors made by the previous models. The algorithm starts with an initial model that makes predictions, and then subsequent models are trained to predict the residual errors of the previous models. The predictions of all the models are summed to obtain the final prediction.\n",
    "\n",
    "The \"gradient\" in gradient boosting refers to the optimization process used to train the ensemble. The algorithm minimizes a loss function by iteratively updating the parameters of the weak models. In each iteration, the algorithm computes the gradients of the loss function with respect to the predictions of the ensemble and uses these gradients to update the parameters of the weak models. The learning process continues until the loss function is minimized or a predefined stopping criterion is met.\n",
    "\n",
    "Gradient boosting regression has several advantages. It can handle a variety of data types, including numerical and categorical features, and it automatically handles feature interactions. It is also robust to outliers and can capture nonlinear relationships between the features and the target variable. However, gradient boosting regression can be computationally expensive and prone to overfitting if not properly regularized.\n",
    "\n",
    "Overall, gradient boosting regression is a powerful algorithm for regression tasks that has been widely used in various domains, including finance, healthcare, and online advertising, due to its accuracy and flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab389d-a8d0-43bf-a48e-a5b06acc258c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9da270c5-6971-49f3-83ce-a924fd4fa398",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "Ans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d312176a-2f7b-492b-a6c1-7291d6adcfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.residuals = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "        self.residuals = []\n",
    "        prediction = np.mean(y)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            residual = y - prediction\n",
    "            tree = self.build_tree(X, residual, depth=0)\n",
    "            self.models.append(tree)\n",
    "            self.residuals.append(residual)\n",
    "            prediction += self.learning_rate * self.predict(X, tree)\n",
    "            \n",
    "    def build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or len(X) <= 1:\n",
    "            return np.mean(y)\n",
    "        \n",
    "        best_feature, best_threshold = self.find_best_split(X, y)\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "        \n",
    "        left_tree = self.build_tree(X[left_indices], y[left_indices], depth+1)\n",
    "        right_tree = self.build_tree(X[right_indices], y[right_indices], depth+1)\n",
    "        \n",
    "        return {'feature': best_feature, 'threshold': best_threshold, 'left': left_tree, 'right': right_tree}\n",
    "    \n",
    "    def find_best_split(self, X, y):\n",
    "        best_mse = np.inf\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "                \n",
    "                left_mse = self.mean_squared_error(y[left_indices])\n",
    "                right_mse = self.mean_squared_error(y[right_indices])\n",
    "                \n",
    "                mse = left_mse + right_mse\n",
    "                \n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def predict(self, X, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        \n",
    "        if X[tree['feature']] <= tree['threshold']:\n",
    "            return self.predict(X, tree['left'])\n",
    "        else:\n",
    "            return self.predict(X, tree['right'])\n",
    "        \n",
    "    def mean_squared_error(self, y):\n",
    "        return np.mean((y - np.mean(y)) ** 2)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.predict(X, self.models[0])\n",
    "        \n",
    "        for i in range(1, self.n_estimators):\n",
    "            y_pred += self.learning_rate * self.predict(X, self.models[i])\n",
    "        \n",
    "        mse = self.mean_squared_error(y)\n",
    "        r_squared = 1 - mse / self.mean_squared_error(y_pred)\n",
    "        \n",
    "        return mse, r_squared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50f251b9-85bc-4a55-8ac9-42b3397df614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.6666666666666665\n",
      "R-squared: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_712/3338873159.py:79: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  r_squared = 1 - mse / self.mean_squared_error(y_pred)\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "X_test = np.array([[6], [7], [8]])\n",
    "y_test = np.array([12, 14, 16])\n",
    "\n",
    "# Create and train the model\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "mse, r_squared = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d129a1-9991-4ca8-8f8f-3a0ebb3d063a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03ea9fb8-b04a-40cf-aa13-0154c216a314",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "\n",
    "Ans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da1ae02e-b654-4dde-982c-620c20f3ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\n",
      "Mean Squared Error: 5403.056937177599\n",
      "R-squared: 0.7430316934906387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59da719-8936-4a8e-a6ed-8b54c9be4476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d95cd065-63ba-420a-98d1-0a72eba3855e",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "Ans.\n",
    "\n",
    "In the context of gradient boosting, a weak learner refers to a simple, relatively low-complexity model that performs slightly better than random guessing on a given task. It is a crucial component of the gradient boosting algorithm as it forms the basis for constructing the ensemble of models.\n",
    "\n",
    "Typically, decision trees are used as weak learners in gradient boosting. These decision trees are often shallow, meaning they have a limited depth or number of splits. Shallow decision trees are computationally efficient and less prone to overfitting.\n",
    "\n",
    "During the training process of gradient boosting, weak learners are sequentially added to the ensemble, and each subsequent learner is trained to correct the errors made by the previous learners. The idea is to combine the predictions of multiple weak learners to create a strong overall model.\n",
    "\n",
    "By themselves, weak learners may not have high predictive power, but their combination through boosting improves the overall model's performance. The iterative nature of gradient boosting, where each weak learner focuses on correcting the mistakes of previous learners, allows the model to progressively learn complex patterns and make accurate predictions.\n",
    "\n",
    "The term \"weak\" in weak learner does not imply that the model is necessarily weak in terms of performance but rather indicates that it performs slightly better than random guessing. The strength of the overall model comes from the ensemble of these weak learners, which collectively learn complex relationships and provide a powerful prediction capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd61b31-112c-48aa-9e27-ae3a4576cc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85c5a84b-a62a-4c2f-9e73-c3ff14551f31",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Ans.\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "- Sequential Improvement: Gradient Boosting aims to improve the predictions iteratively by adding models to the ensemble. It starts with an initial model that makes predictions, and each subsequent model is trained to correct the errors made by the previous models. The predictions of all the models are combined to obtain the final prediction.\n",
    "\n",
    "- Gradient-Based Optimization: The \"gradient\" in Gradient Boosting refers to the optimization process used to train the ensemble. In each iteration, the algorithm calculates the gradients of the loss function with respect to the predictions of the ensemble. These gradients indicate the direction and magnitude of the errors that need to be corrected. The subsequent models are trained to minimize the loss function by updating their parameters based on these gradients.\n",
    "\n",
    "- Focus on Residual Errors: Gradient Boosting focuses on minimizing the residual errors or the differences between the predicted and actual values. Each subsequent model in the ensemble is trained to predict the residual errors of the previous models. By continuously reducing the residuals, the algorithm aims to refine and improve the overall predictions.\n",
    "\n",
    "- Ensemble of Weak Learners: Gradient Boosting combines an ensemble of weak learners, typically decision trees, to create a strong predictive model. The weak learners are typically shallow decision trees with limited depth or splits. These weak learners are iteratively added to the ensemble, and their predictions are weighted and combined to produce the final prediction.\n",
    "\n",
    "- Reduction of Bias and Variance: Gradient Boosting strikes a balance between reducing bias and variance. The sequential addition of models helps reduce bias by iteratively correcting the errors. Moreover, the ensemble of weak learners helps reduce variance by combining the predictions of multiple models and reducing the impact of individual model's shortcomings.\n",
    "\n",
    "- Feature Interaction and Non-linearity: Gradient Boosting is capable of capturing complex feature interactions and non-linear relationships in the data. The ensemble of weak learners can handle a variety of data types, including numerical and categorical features, and automatically capture intricate patterns present in the data.\n",
    "\n",
    "The intuition behind Gradient Boosting lies in its ability to iteratively learn from the mistakes of the previous models and gradually improve the predictions by focusing on the residuals. By combining the predictions of an ensemble of weak learners, Gradient Boosting is able to build a powerful model that can handle complex relationships and provide accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a551f5-053f-403f-aede-44ca67359051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a416c81-f5dd-40da-86c2-22a14fb7488e",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Ans.\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners, typically decision trees, in a sequential manner. Here's a step-by-step explanation of how the ensemble is constructed:\n",
    "\n",
    "- Initialization: The algorithm starts by initializing the ensemble with a simple model. This could be a constant value or the mean of the target variable. The initial model makes predictions for all instances in the dataset.\n",
    "\n",
    "- Compute Residuals: The residuals are calculated as the difference between the actual target values and the predictions made by the current ensemble. The residuals represent the errors that need to be corrected by the subsequent weak learners.\n",
    "\n",
    "- Build Weak Learner: A weak learner, typically a decision tree, is trained to predict the residuals. The decision tree is usually shallow, limiting its depth or number of splits, to avoid overfitting. The weak learner is trained on the features of the dataset using the residuals as the target variable.\n",
    "\n",
    "- Update Ensemble: The predictions made by the weak learner are multiplied by a learning rate and added to the predictions made by the previous models in the ensemble. This update step helps gradually improve the overall predictions.\n",
    "\n",
    "- Iterate: Steps 2-4 are repeated for a specified number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained to predict the residuals, and its predictions are added to the ensemble.\n",
    "\n",
    "- Final Prediction: The final prediction is obtained by summing the predictions made by all the weak learners in the ensemble. The learning rate determines the contribution of each weak learner to the final prediction.\n",
    "\n",
    "By iteratively training weak learners to predict the residuals and updating the ensemble with their predictions, the Gradient Boosting algorithm gradually improves the overall model's performance. The weak learners focus on correcting the errors made by the previous models, allowing the ensemble to learn complex patterns and make accurate predictions.\n",
    "\n",
    "The iterative nature of Gradient Boosting and the sequential training of weak learners distinguish it from other ensemble methods like Random Forest, where the weak learners are trained independently. The sequential training process of Gradient Boosting enables it to build a strong ensemble that combines the predictions of multiple weak learners to make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba5a2f-cbbb-4e13-8248-b2db0dfdf619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deef736a-4072-420f-82da-4e38d00d8050",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves several key steps. Here is an overview of the main steps involved:\n",
    "\n",
    "- Define the Loss Function: The first step is to define the loss function that quantifies the difference between the predicted values and the true values. The choice of the loss function depends on the specific problem and the type of data. Common examples include mean squared error for regression problems and log loss for binary classification problems.\n",
    "\n",
    "- Initialize the Ensemble: The algorithm starts by initializing the ensemble with a simple model. This initial model can be a constant value or the mean of the target variable, depending on the loss function. The initial model makes predictions for all instances in the dataset.\n",
    "\n",
    "- Calculate the Negative Gradient: The negative gradient of the loss function with respect to the current predictions is calculated. This negative gradient represents the direction and magnitude of the errors that need to be corrected by the subsequent weak learners. For example, in regression problems with mean squared error loss, the negative gradient is equal to the differences between the true values and the current predictions.\n",
    "\n",
    "- Train a Weak Learner: A weak learner, typically a decision tree, is trained to predict the negative gradient. The weak learner is trained on the features of the dataset using the negative gradient as the target variable. The decision tree is usually shallow, with limited depth or splits, to avoid overfitting.\n",
    "\n",
    "- Update the Ensemble: The predictions made by the weak learner are multiplied by a learning rate and added to the predictions made by the previous models in the ensemble. This update step helps gradually improve the overall predictions. The learning rate determines the contribution of each weak learner to the final prediction.\n",
    "\n",
    "- Repeat Steps 3-5: Steps 3-5 are repeated for a specified number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained to predict the negative gradient, and its predictions are added to the ensemble.\n",
    "\n",
    "- Final Prediction: The final prediction is obtained by summing the predictions made by all the weak learners in the ensemble. The learning rate determines the contribution of each weak learner to the final prediction.\n",
    "\n",
    "By iteratively training weak learners to predict the negative gradient and updating the ensemble with their predictions, the Gradient Boosting algorithm gradually improves the overall model's performance. The negative gradient drives the learning process, allowing the ensemble to learn complex patterns and make accurate predictions.\n",
    "\n",
    "It's important to note that the mathematical formulation of Gradient Boosting can vary depending on the specific loss function and the implementation details. The steps outlined above provide a general understanding of the core concepts involved in constructing the mathematical intuition of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860ba28-fd91-4803-8398-2304bee9c84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

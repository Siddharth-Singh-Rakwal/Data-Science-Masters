{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fcd5169-3c11-45ee-8137-0756160c2c7e",
   "metadata": {},
   "source": [
    "# Assignment | 10th April 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c31ed-9058-4c0f-8bb8-b85107018db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e343474-0b8f-448c-90e1-afb289fd54b5",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "Ans.\n",
    "\n",
    "To determine the probability that an employee is a smoker given that they use the health insurance plan, we can use Bayes' theorem. Let's denote the following probabilities:\n",
    "\n",
    "P(S) = Probability of being a smoker\n",
    "\n",
    "P(H) = Probability of using the health insurance plan\n",
    "\n",
    "P(S|H) = Probability of being a smoker given that the employee uses the health insurance plan\n",
    "\n",
    "From the given information, we have:\n",
    "\n",
    "P(H) = 0.70 (70% of the employees use the health insurance plan)\n",
    "\n",
    "P(S|H) = 0.40 (40% of the employees who use the plan are smokers)\n",
    "\n",
    "We want to find P(S|H), which can be calculated using Bayes' theorem:\n",
    "\n",
    "P(S|H) = (P(H|S) * P(S)) / P(H)\n",
    "\n",
    "To find P(H|S), the probability of using the health insurance plan given that the employee is a smoker, we need to use the formula:\n",
    "\n",
    "P(H|S) = (P(S|H) * P(H)) / P(S)\n",
    "\n",
    "Now, we can calculate P(H|S):\n",
    "\n",
    "P(H|S) = (0.40 * 0.70) / P(S)\n",
    "\n",
    "Given that P(H) = 0.70 and P(H|S) = 0.40, we can solve for P(S) using the formula:\n",
    "\n",
    "P(H) = P(H|S) * P(S) + P(H|~S) * P(~S)\n",
    "\n",
    "Since P(H|~S) is not provided, we cannot directly calculate P(S). However, we can still determine the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "Without additional information, we cannot determine the exact value of P(S|H)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428638a-1884-4ed8-afa0-fd9352d208e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a270e76-5dbc-4754-a861-470b65d2f509",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm, which is a popular and simple probabilistic classification algorithm.\n",
    "\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the types of features they handle and the assumptions they make about the underlying data.\n",
    "\n",
    "1. Bernoulli Naive Bayes:\n",
    "\n",
    "- Bernoulli Naive Bayes is suitable for binary features, where each feature can take on only two values (usually 0 and 1).\n",
    "- It assumes that each feature is conditionally independent of all other features, given the class variable.\n",
    "- It works well with features that represent the presence or absence of certain characteristics.\n",
    "For example, in text classification, each feature could represent the presence or absence of a specific word in a document.\n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "\n",
    "- Multinomial Naive Bayes is suitable for features that represent discrete counts, such as word frequencies or occurrence counts in text data.\n",
    "- It assumes that the features are generated from a multinomial distribution.\n",
    "- It also assumes that the occurrence of one feature does not affect the occurrence of other features.\n",
    "- It is commonly used in text classification tasks, where the features are often represented as word frequencies or occurrence counts.\n",
    "\n",
    "In summary, the key difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of features they handle and the assumptions they make about the data. Bernoulli Naive Bayes is suitable for binary features, while Multinomial Naive Bayes is suitable for discrete count features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c039c-1049-42d8-a916-a6685cfe9cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58ea6142-9fda-46a1-8cac-dec933638d44",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Bernoulli Naive Bayes, like other Naive Bayes variants, requires complete data without missing values in order to make accurate predictions. However, if the dataset contains missing values, there are a few approaches to handle them in the context of Bernoulli Naive Bayes:\n",
    "\n",
    "1. Deleting instances with missing values: One straightforward approach is to remove instances (rows) that have missing values. This can be done if the missing values are relatively few and random, without significant bias. However, this approach can lead to information loss if the deleted instances contain valuable information for the classification task.\n",
    "\n",
    "2. Imputation: Another approach is to impute missing values with suitable replacements. For Bernoulli Naive Bayes, which deals with binary features, the missing values can be imputed with either 0 or 1, depending on the specific imputation strategy chosen. Common imputation techniques include replacing missing values with the mean, mode, or using more sophisticated methods like regression-based imputation or K-nearest neighbors imputation.\n",
    "\n",
    "It is important to note that the choice of handling missing values in Bernoulli Naive Bayes depends on the specific characteristics of the dataset, the amount of missing data, and the nature of the missingness. It is always recommended to carefully analyze the impact of missing values on the data and choose the most appropriate approach for imputation or handling missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74090b-43b9-464c-889a-42706d18b758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "786cafc3-efc8-4fb5-88b7-8ce25670ec29",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Naive Bayes algorithms, including Gaussian Naive Bayes, can be extended to handle multiple classes by utilizing a one-vs-all (or one-vs-rest) approach.\n",
    "\n",
    "In the context of multi-class classification, Gaussian Naive Bayes assumes that the features follow a Gaussian (normal) distribution within each class. To train a Gaussian Naive Bayes classifier for multi-class problems, the following steps can be followed:\n",
    "\n",
    "1. Training:\n",
    "\n",
    "- For each class in the dataset, calculate the class prior probability, which is the proportion of instances belonging to that class in the training set.\n",
    "- Estimate the mean and variance for each feature within each class. This involves calculating the mean and variance of the feature values for each class separately.\n",
    "\n",
    "2. Prediction:\n",
    "\n",
    "- Given a new instance with feature values, calculate the posterior probability for each class using Bayes' theorem, which involves multiplying the class prior probability with the likelihood of the features given the class (estimated using Gaussian distribution parameters).\n",
    "- The class with the highest posterior probability is assigned as the predicted class for the instance.\n",
    "\n",
    "By using the one-vs-all approach, a separate Gaussian Naive Bayes classifier is trained for each class, treating it as the positive class and the remaining classes as the negative class. During prediction, the instance is classified based on the highest posterior probability obtained from each classifier.\n",
    "\n",
    "It's worth noting that Gaussian Naive Bayes assumes independence between features given the class, which might not always hold in real-world scenarios. Nevertheless, it is a simple and computationally efficient algorithm that can be used for multi-class classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89732866-f149-424c-b21b-3733a51392f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4df9ddf0-0114-4069-a92c-f8e5720b39d9",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "\n",
    "1.  Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "2. Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "3. Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "\n",
    "4. Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "5. Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e354fa19-6fde-44ae-b0fd-d216375b2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd2fdc9-41fa-4bb2-9433-1a8c5ff4bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset:\n",
    "\n",
    "data = np.loadtxt('spambase.data', delimiter=',')\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8432d737-e2ce-4377-a56e-96c5b0bda9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifiers:\n",
    "\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3635a99-4714-46b0-975a-cae828648bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 10-fold cross-validation and compute the performance metrics for each classifier:\n",
    "\n",
    "def evaluate_classifier(classifier, X, y):\n",
    "    accuracy = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')\n",
    "    precision = cross_val_score(classifier, X, y, cv=10, scoring='precision')\n",
    "    recall = cross_val_score(classifier, X, y, cv=10, scoring='recall')\n",
    "    f1 = cross_val_score(classifier, X, y, cv=10, scoring='f1')\n",
    "\n",
    "    return np.mean(accuracy), np.mean(precision), np.mean(recall), np.mean(f1)\n",
    "\n",
    "# Evaluate Bernoulli Naive Bayes\n",
    "accuracy_b, precision_b, recall_b, f1_b = evaluate_classifier(bernoulli_nb, X, y)\n",
    "\n",
    "# Evaluate Multinomial Naive Bayes\n",
    "accuracy_m, precision_m, recall_m, f1_m = evaluate_classifier(multinomial_nb, X, y)\n",
    "\n",
    "# Evaluate Gaussian Naive Bayes\n",
    "accuracy_g, precision_g, recall_g, f1_g = evaluate_classifier(gaussian_nb, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f1135b-9183-4ea2-b6e5-229f2c5f2cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8869617393737383\n",
      "Recall: 0.8152389047416673\n",
      "F1 Score: 0.8481249015095276\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7393175533565436\n",
      "Recall: 0.7214983911116508\n",
      "F1 Score: 0.7282909724016348\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7103733928118492\n",
      "Recall: 0.9569516119239877\n",
      "F1 Score: 0.8130660909542995\n"
     ]
    }
   ],
   "source": [
    "# Print the performance metrics:\n",
    "\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_b)\n",
    "print(\"Precision:\", precision_b)\n",
    "print(\"Recall:\", recall_b)\n",
    "print(\"F1 Score:\", f1_b)\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_m)\n",
    "print(\"Precision:\", precision_m)\n",
    "print(\"Recall:\", recall_m)\n",
    "print(\"F1 Score:\", f1_m)\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_g)\n",
    "print(\"Precision:\", precision_g)\n",
    "print(\"Recall:\", recall_g)\n",
    "print(\"F1 Score:\", f1_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e3dbe-ee27-49cd-928e-66accccf55aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14ec6f46-d57f-4e93-820d-52ca98ba3568",
   "metadata": {},
   "source": [
    "- Discussion:\n",
    "\n",
    "Based on the results obtained, we can analyze the performance of each variant of Naive Bayes. The Bernoulli Naive Bayes variant models the presence or absence of a feature, assuming a binary distribution. It performs well when dealing with binary data, such as whether a certain word is present in an email or not. Multinomial Naive Bayes, on the other hand, assumes a multinomial distribution and is suitable for discrete data, like word counts. Finally, Gaussian Naive Bayes assumes a Gaussian distribution and is suitable for continuous numerical data.\n",
    "\n",
    "In this case, we can expect Bernoulli Naive Bayes to perform well since the dataset represents email messages where the presence or absence of certain words might be indicative of spam. However, it is essential to analyze the results to determine the best-performing variant.\n",
    "\n",
    "The limitations of Naive Bayes include its assumption of feature independence, which might not hold in some cases. Additionally, Naive Bayes can struggle with data sparsity, as it calculates probabilities based on observed frequencies. In the case of the spambase dataset, the performance of Naive Bayes can be affected by these limitations.\n",
    "\n",
    "- Conclusion:\n",
    "\n",
    "In conclusion, we implemented Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers on the spambase dataset and evaluated their performance using 10-fold cross-validation. By comparing the accuracy, precision, recall, and F1 scores, we can determine which variant of Naive Bayes performed the best. Additionally, we discussed the limitations of Naive Bayes, which should be considered when applying this algorithm to real-world problems. Suggestions for future work could include exploring other classification algorithms and feature engineering techniques to improve the performance on the spambase dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7825ada-da79-4e5f-b74e-40ab0bea1108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
